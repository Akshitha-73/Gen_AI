{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OL3zYhMuLqB",
        "outputId": "1b24d380-1373-4571-978c-189dfd74676c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/144.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U -q \"google-genai==1.7.0\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types"
      ],
      "metadata": {
        "id": "BgnFYXXMuXGK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMnaSxRA1NWF",
        "outputId": "90523e4b-4221-4fc8-e77f-87494feec349"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key='/content/apikey.txt'\n"
      ],
      "metadata": {
        "id": "naj6WmYG1PZF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_key_value(filepath, key_name):\n",
        "    with open(filepath, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.strip().startswith(f\"{key_name}=\"):\n",
        "                # Extract the value and remove leading/trailing whitespace and quotes\n",
        "                value = line.strip().split('=', 1)[1].strip()\n",
        "                if value.startswith('\"') and value.endswith('\"'):\n",
        "                    return value[1:-1]\n",
        "\n",
        "    return None\n",
        "\n",
        "api_kru = extract_key_value(api_key, 'API_KEY')\n"
      ],
      "metadata": {
        "id": "51imKK8JuxcG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if api_kru is None:\n",
        "    print(f\"Error: API key 'API_KEY' not found in {api_key}\")\n",
        "else:\n",
        "    client=genai.Client(api_key=api_kru)\n",
        "\n",
        "    response= client.models.generate_content(\n",
        "        model='gemini-2.0-flash',\n",
        "        contents=\"exaplin agentic AI to me like I'm a kid.\"\n",
        "\n",
        "    )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hW2oTe6I1xyz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "a4LoMnNm4n5P",
        "outputId": "3cce6139-e796-4060-d757-13c96a0848a7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, imagine you have a really smart toy robot. Most robots just do exactly what you tell them, like \"move forward\" or \"turn left.\"\n",
            "\n",
            "But an **Agentic AI** robot is different! It's like a robot that can **think for itself** and **make its own plans** to reach a goal you give it.\n",
            "\n",
            "Imagine you tell your robot: \"Make me a peanut butter and jelly sandwich.\"\n",
            "\n",
            "*   A regular robot would just stand there because you didn't tell it all the steps.\n",
            "\n",
            "*   But an Agentic AI robot would:\n",
            "    *   **Figure out** it needs bread, peanut butter, and jelly.\n",
            "    *   **Decide** to go to the kitchen.\n",
            "    *   **Look** in the cabinets and refrigerator for the ingredients.\n",
            "    *   **Use** a knife to spread the peanut butter and jelly.\n",
            "    *   **Put** the sandwich together.\n",
            "    *   **Bring** it to you!\n",
            "\n",
            "So, Agentic AI is like giving your robot a mission, and it **figures out all the steps** on its own, just like a detective solving a case or a smart little helper doing chores. It can even **learn** from its mistakes and do better next time!\n",
            "\n",
            "It's not magic, it's just really smart computer programs that can plan, decide, and act. It's like giving a computer the ability to be a little bit more like a human!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-generativeai\n"
      ],
      "metadata": {
        "id": "-pjJYl148cZA"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "\n",
        "genai.configure(api_key=api_kru)\n",
        "\n",
        "# Load the Gemini 1.5 Flash model\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n"
      ],
      "metadata": {
        "id": "-yxRLskE9QMX"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"summarize agentic AI in 3 bullet points\"\n",
        "\n",
        "response=model.generate_content(prompt)\n"
      ],
      "metadata": {
        "id": "GOiYUglE9Wm2"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Vao03ON9fmu",
        "outputId": "7318d4a5-48dc-4a87-f4f4-5a1255d2d71f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* **Goal-oriented:** Agentic AI systems are characterized by their ability to autonomously pursue goals and objectives in dynamic environments.  They don't just react to inputs; they actively plan and act to achieve desired outcomes.\n",
            "\n",
            "* **Proactive and adaptive:** Unlike reactive AI, which responds only to immediate stimuli, agentic AI anticipates future states, plans accordingly, and adapts its actions based on feedback and changing circumstances.\n",
            "\n",
            "* **Autonomous decision-making:** Agentic AI systems possess the capacity to make independent decisions without constant human intervention. This autonomy extends to selecting actions, prioritizing tasks, and resolving conflicts to achieve their goals.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='''what are different parameters in chats.create(model,)'''\n",
        "\n",
        "resp=model.generate_content(prompt)"
      ],
      "metadata": {
        "id": "4Ynp0epZ9oHg"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(resp.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ouy0bj6D-O8t",
        "outputId": "44cfbfe9-d965-4e50-fbd3-838dad37d358"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The `chats.create()` method (presumably referring to an API call, likely from a platform like OpenAI or a similar service)  doesn't have a universally standardized parameter list.  The exact parameters depend heavily on the specific API provider and version.  However, some common parameters you'll frequently encounter include:\n",
            "\n",
            "**Essential Parameters (Almost always required):**\n",
            "\n",
            "* **`model`:**  This specifies the language model to use for the chat.  This is crucial and determines the capabilities and style of the response.  Options might include different versions of GPT,  Llama, etc., each with varying strengths and costs.\n",
            "\n",
            "* **`messages`:** This is usually an array of objects, where each object represents a message in the conversation. Each message object typically contains at least:\n",
            "    * `role`:  Indicates the sender of the message (\"system\", \"user\", \"assistant\").\n",
            "    * `content`: The actual text of the message.\n",
            "\n",
            "**Common Optional Parameters:**\n",
            "\n",
            "* **`temperature` (or similar):** Controls the randomness of the model's output.  A higher temperature (e.g., 0.8) leads to more creative and unpredictable responses, while a lower temperature (e.g., 0.2) produces more focused and deterministic outputs.\n",
            "\n",
            "* **`max_tokens`:** Limits the length of the model's response in tokens (roughly equivalent to words). This helps control costs and prevents excessively long replies.\n",
            "\n",
            "* **`top_p` (or `nucleus_sampling`):**  Similar to temperature, but instead of controlling randomness directly, it controls the cumulative probability mass considered for sampling the next token.  Often used in conjunction with temperature.\n",
            "\n",
            "* **`frequency_penalty`:**  A penalty applied to tokens that have already appeared frequently in the conversation.  Higher values discourage repetition.\n",
            "\n",
            "* **`presence_penalty`:** A penalty applied to tokens that have already appeared in the conversation (regardless of frequency).  Higher values discourage the model from revisiting previously discussed topics.\n",
            "\n",
            "* **`stop`:** A list of strings that, if encountered in the model's response, will cause the generation to stop prematurely.  Useful for controlling the length or topic of the response.\n",
            "\n",
            "\n",
            "* **`n`:**  Number of independent responses to generate.  If `n` > 1, you get multiple different outputs for the same prompt.\n",
            "\n",
            "\n",
            "* **`stream`:**  If `true`, the response is streamed back in chunks, allowing for real-time display of the generated text.  (This is crucial for interactive chat experiences.)\n",
            "\n",
            "\n",
            "* **`user`:**  An identifier for the user initiating the chat.  Useful for tracking conversations and maintaining context across multiple requests.\n",
            "\n",
            "\n",
            "\n",
            "**Less Common or API-Specific Parameters:**\n",
            "\n",
            "Many APIs may offer additional parameters for more fine-grained control over the generation process.  These could include parameters related to:\n",
            "\n",
            "* **Cost management:**  Limiting the cost of the request.\n",
            "* **Context window size:**  Setting a limit on the length of the conversation history considered by the model.\n",
            "* **Model-specific options:**  Parameters specific to the chosen language model.\n",
            "\n",
            "\n",
            "**Important Note:** You *must* consult the official documentation for the specific chat API you're using.  The exact names, types, and availability of parameters will vary. The examples above are general guidelines, not a definitive list.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat=client.chats.create(model='gemini-2.0-flash',history=[])\n",
        "\n"
      ],
      "metadata": {
        "id": "im_oCGfd-SOw"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp=chat.send_message('hello! my name akshitha')\n",
        "\n",
        "print(resp.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESIlKwud-fn8",
        "outputId": "f96e23cd-31ee-45ec-f68f-9cd0488976e2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Akshitha! It's nice to meet you. How can I help you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re=chat.get_history()\n",
        "print(re)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE76gbOO-0to",
        "outputId": "0c6ad0be-11a7-42d9-8298-0778712af9db"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UserContent(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='hello! my name akshitha')], role='user'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"Hello Akshitha! It's nice to meet you. How can I help you today?\\n\")], role='model')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re=chat.send_message_stream('what is capital of telangana')\n",
        "for chunk in re:\n",
        "    print(chunk.text, end='')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG9e39qu_57x",
        "outputId": "6947f316-e43a-4731-98d0-bbb0ed78034b"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of Telangana is **Hyderabad**.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## explore generation parameters\n"
      ],
      "metadata": {
        "id": "GwCGl6SoBF-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### output length"
      ],
      "metadata": {
        "id": "c7BaQPCfBJD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configure=types.GenerateContentConfig(max_output_tokens=200\n",
        "                                      )"
      ],
      "metadata": {
        "id": "uULETMFGFQ4X"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=configure,\n",
        "    contents='what 250 words on actor prabhas'\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "XSz9PWuFAzrL"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSOR2ttsFzSs",
        "outputId": "ff8c9993-fea5-4791-c14b-3162c902dd7d"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prabhas, born Uppalapati Venkata Suryanarayana Prabhas Raju in 1979, is an Indian actor primarily known for his work in Telugu cinema. He's become a pan-Indian superstar, transcending regional boundaries with his captivating performances and charismatic screen presence.\n",
            "\n",
            "Prabhas made his acting debut in 2002 with the Telugu film \"Eeswar\" but gained significant recognition with \"Varsham\" in 2004. He further solidified his position with films like \"Chatrapathi\" and \"Darling,\" establishing him as a bankable leading man.\n",
            "\n",
            "However, Prabhas' career reached unprecedented heights with S.S. Rajamouli's epic historical fiction film \"Baahubali: The Beginning\" (2015) and its sequel \"Baahubali 2: The Conclusion\" (2017). He dedicated five years to these films, showcasing incredible dedication and transforming himself physically. His\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### temperature"
      ],
      "metadata": {
        "id": "-I2pOMDUGDbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp_config=types.GenerateContentConfig(temperature=0.5)\n"
      ],
      "metadata": {
        "id": "P55CffdYF-xa"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  rep=client.models.generate_content(\n",
        "      model='gemini-2.0-flash',\n",
        "      config=temp_config,\n",
        "      contents='pick a random new generation tollywood actor name...(respond in a single word)')\n",
        "  if rep.text:\n",
        "    print(rep.text, '-' * 25)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lXdwgR9F162",
        "outputId": "ec500a49-b7bb-494b-ed62-fa5af4548990"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vishwak Sen\n",
            " -------------------------\n",
            "Vishwak Sen\n",
            " -------------------------\n",
            "Vishwak Sen\n",
            " -------------------------\n",
            "Vishwak Sen\n",
            " -------------------------\n",
            "Vishwak Sen\n",
            " -------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rep.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "j2dABT0sGusK",
        "outputId": "32bde3ee-9ae8-4beb-e11f-cdc91ff7a092"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Vishwak Sen\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_config=types.GenerateContentConfig(temperature=0)"
      ],
      "metadata": {
        "id": "P-cWIT9-HfSd"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### top-p"
      ],
      "metadata": {
        "id": "dNihdgjkIOj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = types.GenerateContentConfig(\n",
        "    # These are the default values for gemini-2.0-flash.\n",
        "    temperature=1.0,\n",
        "    top_p=0.95,\n",
        ")\n",
        "\n",
        "story_prompt = \"You are a creative writer.write a short story on earth\"\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=model_config,\n",
        "    contents=story_prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBqrnOiSIHCy",
        "outputId": "d5a038d5-1b27-4ed5-ee7e-6298a43b53df"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The old woman, Gaia, sighed. It was a sound that rustled through redwood forests and whispered across the Sahara. Her bones, the tectonic plates, ached. She felt the tremors building, a restless energy in her molten core.\n",
            "\n",
            "She wasn’t exactly human, though humans were her most favored, and most troublesome, children. She was Earth. She was the swirling blue marble, the rustling green forests, the icy breath of the poles. She felt everything: the relentless sun baking her skin, the sting of acid rain, the frantic, panicked vibration of a dying whale tangled in plastic.\n",
            "\n",
            "For millennia, she had watched civilizations rise and fall, bloom and wither like desert flowers. She had patiently nurtured life, coaxed it from the primordial soup, guided it through ice ages and meteor strikes. She’d even chuckled, a rumble that echoed through mountain ranges, when the dinosaurs went the way of the dodo.\n",
            "\n",
            "But now… now she felt a different kind of exhaustion. A bone-deep weariness that settled heavier than any glacier.\n",
            "\n",
            "The humans, her clever, inventive, destructive darlings, had become a blight. They consumed her resources with a voracity that frightened even her. They poisoned her waters, choked her air, and waged wars upon themselves in the name of… what? Profit? Power?\n",
            "\n",
            "Gaia shifted, a groaning sound that manifested as a minor earthquake in Nepal. She remembered a time when humans lived in harmony with her, drawing only what they needed, respecting the rhythms of her cycles. They sang to the moon, thanked the river, and whispered apologies to the trees they felled.\n",
            "\n",
            "Now, their songs were drowned out by the roar of machines, their gratitude replaced with greed, their whispers lost in the cacophony of consumption.\n",
            "\n",
            "She had tried to nudge them, of course. Gentle reminders. A hurricane here, a drought there. But they interpreted her warnings as acts of God, or worse, dismissed them as inconvenient weather.\n",
            "\n",
            "She could wipe them out, of course. A good, clean cleansing. A giant meteor, a supervolcano, another ice age. The planet would heal. Life, in some form, would find a way.\n",
            "\n",
            "But… she hesitated. They were still *her* children. Even the most rebellious child deserved a second chance, didn't they?\n",
            "\n",
            "Gaia focused her will, a subtle manipulation of weather patterns, a gentle push towards renewable energy. She nudged the hearts of scientists, artists, and activists, planting seeds of compassion and innovation.\n",
            "\n",
            "It was a small start, a whisper in the wind. She knew the odds were stacked against her. But as she watched a lone tree begin to sprout from a concrete crack in a forgotten city, she felt a flicker of hope.\n",
            "\n",
            "Perhaps, just perhaps, these flawed, beautiful, destructive creatures could still learn to dance to the rhythm of her heart. Perhaps they could still remember what it meant to be a part of her, a part of Earth.\n",
            "\n",
            "The sigh that followed this time was different. It wasn't weary. It was a quiet, hopeful breath. The earth held its breath too, waiting to see what its children would do. The future, as always, remained to be written, etched in the sands of time, whispered on the wind, and held within the heart of the old woman, Gaia.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## prompting\n"
      ],
      "metadata": {
        "id": "RjVmhgCGI1yJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### zero-shot\n"
      ],
      "metadata": {
        "id": "t4xuhFhjI4h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configure=types.GenerateContentConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=1,\n",
        "    max_output_tokens=5,\n",
        ")\n"
      ],
      "metadata": {
        "id": "oLPE7JsHIcdc"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Her\" is a disturbing study revealing the direction\n",
        "humanity is headed if AI is allowed to keep evolving,\n",
        "unchecked. I wish there were more movies like this masterpiece.\n",
        "Sentiment: \"\"\""
      ],
      "metadata": {
        "id": "7E0qJj4IJG4v"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res=client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=configure,\n",
        "    contents=zero_shot_prompt\n",
        ")"
      ],
      "metadata": {
        "id": "ba_YfZLfJKwl"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(res.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScUEtqY9JU-i",
        "outputId": "8059b9f4-badd-4de1-9db2-a28d771e74f9"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POSITIVE\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## enum mode"
      ],
      "metadata": {
        "id": "OQDRnNMTS7Vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum"
      ],
      "metadata": {
        "id": "ZPUoiQnwS61f"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re='i movie is sooo dull'"
      ],
      "metadata": {
        "id": "7m3F3bebUhv_"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sentiment(enum.Enum):\n",
        "    POSITIVE = \"positive\"\n",
        "    NEUTRAL = \"neutral\"\n",
        "    NEGATIVE = \"negative\"\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_mime_type=\"text/x.enum\",\n",
        "        response_schema=Sentiment\n",
        "    ),\n",
        "    contents=re\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dUcs3bwJWm4",
        "outputId": "b1c278e8-f1db-4bd2-d2e9-a79c5faa6929"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enum_rep=response.parsed"
      ],
      "metadata": {
        "id": "l2DaLZs4T0-E"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(enum_rep)\n",
        "print(type(enum_rep))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gT4G6FWVUwqs",
        "outputId": "4242a4ea-cc8b-484e-fce5-086570a18aaa"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment.NEGATIVE\n",
            "<enum 'Sentiment'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### one shot and few shot"
      ],
      "metadata": {
        "id": "hKUw7NAAU3Ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples=\"\"\"Parse a customer's pizza order into valid JSON:\n",
        "\n",
        "EXAMPLE:\n",
        "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"small\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
        "}\n",
        "```\n",
        "\n",
        "EXAMPLE:\n",
        "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"large\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
        "}\n",
        "```\n",
        "\n",
        "ORDER:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "SxiZRpb1Uzoj"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cut_order=''' Now, I would like a large pizza, with the first half cheese and\n",
        "mozzarella. And the other tomato sauce, ham and pineapple.'''\n"
      ],
      "metadata": {
        "id": "1kaklj4LVCpg"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rep=client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        temperature=0.1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=100\n",
        "    ),\n",
        "    contents=([examples+cut_order])\n",
        ")"
      ],
      "metadata": {
        "id": "K49TiHtPVa6N"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rep.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ULrHjL8VrSO",
        "outputId": "e1c00140-7738-4b37-d4c0-7c9e7326b532"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "\"size\": \"large\",\n",
            "\"type\": \"half and half\",\n",
            "\"first_half\": [\"cheese\", \"mozzarella\"],\n",
            "\"second_half\": [\"tomato sauce\", \"ham\", \"pineapple\"]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### chain of thought --- step by step"
      ],
      "metadata": {
        "id": "VrRUu1dwWvBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
        "am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=prompt)\n"
      ],
      "metadata": {
        "id": "aBg018evWuHI"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTPt8kmJVtMK",
        "outputId": "9662a2e1-3530-47c5-93be-ef4aa0898f51"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
        "am 20 years old. How old is my partner? Return the step by step\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=prompt)\n"
      ],
      "metadata": {
        "id": "zuQ5AZWuZjU5"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4GA7sEqZpyb",
        "outputId": "d03b2c26-842e-4c4b-d1b4-0d015f35eec5"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's how to solve the problem step-by-step:\n",
            "\n",
            "1. **Find the age difference:** When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n",
            "2. **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n",
            "3. **Determine partner's current age:** Since the age difference remains the same, your partner is always 8 years older than you. Now that you are 20, your partner is 20 + 8 = 28 years old.\n",
            "\n",
            "**Therefore, your partner is 28 years old.**\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## react"
      ],
      "metadata": {
        "id": "BW-1fEACZxJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_instructions = \"\"\"\n",
        "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
        "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
        " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
        "     will return some similar entities to search and you can try to search the information from those topics.\n",
        " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
        "     so keep your searches short.\n",
        " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
        "\"\"\"\n",
        "\n",
        "example1 = \"\"\"Question\n",
        "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
        "\n",
        "Thought 1\n",
        "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
        "\n",
        "Action 1\n",
        "<search>Milhouse</search>\n",
        "\n",
        "Observation 1\n",
        "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
        "\n",
        "Thought 2\n",
        "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
        "\n",
        "Action 2\n",
        "<lookup>named after</lookup>\n",
        "\n",
        "Observation 2\n",
        "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
        "\n",
        "Thought 3\n",
        "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
        "\n",
        "Action 3\n",
        "<finish>Richard Nixon</finish>\n",
        "\"\"\"\n",
        "\n",
        "example2 = \"\"\"Question\n",
        "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
        "\n",
        "Thought 1\n",
        "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
        "\n",
        "Action 1\n",
        "<search>Colorado orogeny</search>\n",
        "\n",
        "Observation 1\n",
        "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
        "\n",
        "Thought 2\n",
        "It does not mention the eastern sector. So I need to look up eastern sector.\n",
        "\n",
        "Action 2\n",
        "<lookup>eastern sector</lookup>\n",
        "\n",
        "Observation 2\n",
        "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
        "\n",
        "Thought 3\n",
        "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
        "\n",
        "Action 3\n",
        "<search>High Plains</search>\n",
        "\n",
        "Observation 3\n",
        "High Plains refers to one of two distinct land regions\n",
        "\n",
        "Thought 4\n",
        "I need to instead search High Plains (United States).\n",
        "\n",
        "Action 4\n",
        "<search>High Plains (United States)</search>\n",
        "\n",
        "Observation 4\n",
        "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
        "\n",
        "Thought 5\n",
        "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
        "\n",
        "Action 5\n",
        "<finish>1,800 to 7,000 ft</finish>\n",
        "\"\"\"\n",
        "\n",
        "# Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/"
      ],
      "metadata": {
        "id": "cBgyNJ5z0VSs",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-19T09:34:42.769953Z",
          "iopub.execute_input": "2025-06-19T09:34:42.770262Z",
          "iopub.status.idle": "2025-06-19T09:34:42.776375Z",
          "shell.execute_reply.started": "2025-06-19T09:34:42.770236Z",
          "shell.execute_reply": "2025-06-19T09:34:42.775277Z"
        }
      },
      "outputs": [],
      "execution_count": 152
    },
    {
      "cell_type": "code",
      "source": [
        "question='what is 21st movie of prabhas?'\n"
      ],
      "metadata": {
        "id": "pvstGPRoZ92L"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "react_config=types.GenerateContentConfig(\n",
        "    stop_sequences=['Action:', 'Observation:'],\n",
        "    system_instruction=model_instructions+example1+example2,\n",
        ")\n",
        ""
      ],
      "metadata": {
        "id": "AAvjUIhpZ9y1"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "react_chat=client.chats.create(\n",
        "    model='gemini-2.0-flash',\n",
        "    history=[]\n",
        ")"
      ],
      "metadata": {
        "id": "u2B34XcgZ9wo"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp=react_chat.send_message(question)"
      ],
      "metadata": {
        "id": "bFofCrtdZ9uZ"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(resp.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ai8k2TUKZ9sU",
        "outputId": "c5676678-0e91-4d67-c7cc-57cb67caa6ca"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As of October 2024, Prabhas's 21st film is titled *Kalki 2898 AD*.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observation=\"\"\"\n",
        "Prabhas then starred in a dual role in the blockbuster epic action duology Baahubali: The Beginning (2015) and its sequel Baahubali 2: The Conclusion (2017), with the latter emerging as the highest-grossing Indian film of all time at that point. Prabhas became the first pan-Indian star and earned international recognition for his performance in the films.[14][15][16][17] He has since starred in the action thriller Saaho (2019), the action drama Salaar: Part 1 – Ceasefire (2023), and the science fiction film Kalki 2898 AD (2024), all of which rank among the highest-grossing Indian films.[18][19]\"\"\""
      ],
      "metadata": {
        "id": "1FkpTm7oaYJA"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp=react_chat.send_message(observation)\n"
      ],
      "metadata": {
        "id": "n_ZyNn_1bN7l"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(resp.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XekQJKCRbR_k",
        "outputId": "7a57ca56-b1dc-4324-b55c-b06fbf3dd34d"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, thanks for providing that information. It seems like you're laying out some of Prabhas's more recent filmography. Is there something specific you want to ask or discuss about it?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## thinking mode"
      ],
      "metadata": {
        "id": "zPkO5oZPbmXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io"
      ],
      "metadata": {
        "id": "DH8zt_b7bWNy"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp=client.models.generate_content_stream(\n",
        "    model='gemini-2.0-flash-thinking-exp',\n",
        "    contents='who was the 2nd female lead in kalki 2898 AD?'\n",
        ")\n",
        "\n",
        "buf=io.StringIO()\n",
        "for i in resp:\n",
        "  buf.write(i.text)\n",
        "  print(i.text, end='')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmOBIgNbbu6-",
        "outputId": "80de8493-289c-469d-e446-a31c71eb864d"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The term \"2nd female lead\" isn't official, but based on the prominence of the roles after the main female lead (Deepika Padukone as Sumathi), **Shobana** is generally considered to have the most significant second female role in the film.\n",
            "\n",
            "She plays the character of **Manasa**.\n",
            "\n",
            "Anna Ben also has a significant female role as Kaira, but Shobana's character Manasa appears to have a more central role in the overarching plot beyond just supporting the main characters' immediate journeys."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## generating code"
      ],
      "metadata": {
        "id": "Oif0bD1Zcyr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code_prompt = \"\"\"\n",
        "Write a Python function to calculate the factorial of a number. no explanation, only give me code\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=1024,\n",
        "    ),\n",
        "    contents=code_prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fryTfZrwcRva",
        "outputId": "f3fcf6ca-a1ac-44b1-e9a7-b1eec7969600"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "def factorial(n):\n",
            "  \"\"\"Calculates the factorial of a number.\"\"\"\n",
            "  if n == 0:\n",
            "    return 1\n",
            "  else:\n",
            "    return n * factorial(n-1)\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## code execution"
      ],
      "metadata": {
        "id": "PYrXaiZDdIII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code_prompt=\"\"\"\n",
        "generate the first 14 odd prime numbers , then calculate their sum.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-Y_17HDAc7yS"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp=client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[\n",
        "            types.Tool(codeExecution=types.ToolCodeExecution())]\n",
        "    ),\n",
        "    contents=code_prompt\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "i8SehQIUc-FF"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for part in resp.candidates[0].content.parts:\n",
        "  print(part.to_json_dict())\n",
        "  print(\"-----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKkXkcgmdt4z",
        "outputId": "3475021f-820c-4f6d-a5cb-65a0776611c6"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'Okay, I can do that. First, I need to identify the first 14 odd prime numbers. Remember that a prime number is a number greater than 1 that has only two factors: 1 and itself. Odd numbers are integers not divisible by 2. Thus, I need to find prime numbers greater than 2. After I find those 14 numbers, I will calculate their sum.\\n\\n'}\n",
            "-----\n",
            "{'executable_code': {'code': \"primes = []\\nnumber = 3\\nwhile len(primes) < 14:\\n    is_prime = True\\n    for i in range(2, int(number**0.5) + 1):\\n        if number % i == 0:\\n            is_prime = False\\n            break\\n    if is_prime:\\n        primes.append(number)\\n    number += 2\\n\\nprint(f'{primes=}')\\n\\nimport numpy as np\\nsum_of_primes = np.sum(primes)\\nprint(f'{sum_of_primes=}')\\n\", 'language': 'PYTHON'}}\n",
            "-----\n",
            "{'code_execution_result': {'outcome': 'OUTCOME_OK', 'output': 'primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\\nsum_of_primes=np.int64(326)\\n'}}\n",
            "-----\n",
            "{'text': 'The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, and 47.\\n\\nTheir sum is 326.\\n'}\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_contents = !curl https://numpy.org/\n",
        "\n",
        "explain_prompt = f\"\"\"\n",
        "Please explain what this file does in detail:\n",
        "\n",
        "```\n",
        "{file_contents}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=explain_prompt)"
      ],
      "metadata": {
        "id": "LOnIlwYbdz3F"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1mSansCeYaU",
        "outputId": "9ddecf93-5350-49e8-de38-c5e7445f5927"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This file is the HTML source code for the **NumPy project's homepage (likely `index.html`)**.  Let's break down what it does, section by section.\n",
            "\n",
            "**1. HTML Structure and Metadata:**\n",
            "\n",
            "*   `<!doctype html><html lang=en data-colorscheme=light>`: Declares the document as HTML5, sets the language to English, and specifies a light color scheme.\n",
            "*   `<head>`: Contains metadata about the page.\n",
            "    *   `<meta name=generator content=\"Hugo 0.141.0\">`: Indicates that the site was generated using Hugo, a static site generator (version 0.141.0). This is a crucial piece of information!  It means this isn't hand-written HTML; it's likely generated from Markdown or other source files.\n",
            "    *   `<meta name=description content=\"...\">`: Provides a short description of NumPy, used by search engines.\n",
            "    *   `<meta charset=utf-8>`: Sets the character encoding to UTF-8 (supports a wide range of characters).\n",
            "    *   `<meta name=viewport content=\"width=device-width,initial-scale=1,shrink-to-fit=no\">`: Configures the viewport for responsive design, ensuring the page scales properly on different devices.\n",
            "    *   `<meta http-equiv=x-ua-compatible content=\"ie=edge\">`: Ensures compatibility with older versions of Internet Explorer.\n",
            "    *   `<title>NumPy</title>`: Sets the title that appears in the browser tab.\n",
            "*   `<body>`:  Contains the visible content of the page.\n",
            "\n",
            "**2. CSS Stylesheets:**\n",
            "\n",
            "A *lot* of `<link rel=stylesheet>` tags are present.  These link the HTML document to various CSS files.  Crucially, look at the `href` attributes.  They point to files like:\n",
            "\n",
            "*   `/theme-css/sphinx-design/index.scss.min.css`\n",
            "*   `/theme-css/pst/bootstrap.scss.min.css`\n",
            "*   `/theme-css/pst/pydata-sphinx-theme.scss.min.css`\n",
            "*   `/theme-css/spht/index.scss.min.css`\n",
            "*   `/css/tabs.scss.min.css`\n",
            "*   `/theme-css/backtotop.min.css`\n",
            "*   `/theme-css/bulma.min.css`\n",
            "*   `/theme-css/code-highlight.min.css`\n",
            "*   `/theme-css/content.min.css`\n",
            "*   `/theme-css/dark-mode.min.css`\n",
            "*   `/theme-css/footer.min.css`\n",
            "*   `/theme-css/hero.min.css`\n",
            "*   `/theme-css/lists.min.css`\n",
            "*   `/theme-css/navbar.min.css`\n",
            "*   `/theme-css/news.min.css`\n",
            "*   `/theme-css/posts.min.css`\n",
            "*   `/theme-css/search.min.css`\n",
            "*   `/theme-css/shortcuts.min.css`\n",
            "*   `/theme-css/styles.min.css`\n",
            "*   `/theme-css/tables.min.css`\n",
            "*   `/theme-css/tabs.min.css`\n",
            "*   `/theme-css/vars.min.css`\n",
            "*   `/css/casestudies.min.css`\n",
            "*   `/css/custom.min.css`\n",
            "*   `/css/mailchimp.min.css`\n",
            "*   `/css/shell.min.css`\n",
            "\n",
            "These CSS files are responsible for the visual appearance of the page. They control layout, colors, fonts, and other styling elements.   The filenames suggest:\n",
            "\n",
            "*   **`sphinx-design`, `pydata-sphinx-theme`:**  These heavily indicate that the site uses the PyData Sphinx Theme.  Sphinx is a documentation generator, and the PyData Sphinx Theme provides a specific look-and-feel tailored to scientific Python projects. This confirms the site is built for documentation and presentation of technical content.\n",
            "*   **`bootstrap`, `bulma`:**  These are CSS frameworks that provide pre-built components and layout tools for easier web development.  It's possible the PyData Sphinx Theme relies on one of these frameworks.\n",
            "*   **`dark-mode`:**  Suggests the site has a dark mode.\n",
            "*   **`tabs`, `navbar`, `footer`, `hero`:** These define the styling of specific page sections.\n",
            "*   **`code-highlight`:**  Handles syntax highlighting for code examples.\n",
            "*   **`.scss.min.css`:**  This indicates that the CSS files were likely written in SCSS (Sass), a CSS preprocessor, and then compiled and minified for better performance.\n",
            "\n",
            "**3. JavaScript:**\n",
            "\n",
            "*   `<script src=https://code.jquery.com/jquery-3.7.1.min.js></script>`: Includes the jQuery JavaScript library, a common tool for manipulating the DOM (Document Object Model), handling events, and making AJAX requests.\n",
            "*   `<script type=text/javascript src=/js/bundle.min.js></script>`:  Includes a custom JavaScript bundle (likely minified). This bundle probably contains the site's interactive elements, such as navigation, animations, or other dynamic features.\n",
            "*   `<script type=text/javascript>setupShortcuts(maxLevel=2)</script>`: A small inline JavaScript snippet to initialize keyboard shortcuts, probably for navigation.\n",
            "*   `<script defer data-domain=numpy.org src=https://views.scientific-python.org/js/script.js></script>`:  Includes a script for website analytics, likely tracking page views and other user interactions. \"scientific-python.org\" suggests this is part of a broader analytics initiative within the scientific Python ecosystem. `defer` means the script will be executed after the HTML is parsed.\n",
            "\n",
            "**4. Language Alternatives:**\n",
            "\n",
            "*   `<link rel=alternate hreflang=pt href=/pt/ title=Português>`\n",
            "*   `<link rel=alternate hreflang=ja href=/ja/ title=\"日本語 (Japanese)\">`\n",
            "*   `<link rel=alternate hreflang=es href=/es/ title=Español>`\n",
            "\n",
            "These links indicate that the site is available in Portuguese, Japanese, and Spanish.  The `hreflang` attribute specifies the language code, and the `href` attribute points to the translated version of the page.\n",
            "\n",
            "**5. Open Graph and Twitter Metadata:**\n",
            "\n",
            "*   `<meta name=twitter:card content=\"summary_large_image\">` and similar `<meta name=twitter:...>` tags:  These are Twitter-specific metadata tags that control how the page is displayed when shared on Twitter.  `summary_large_image` creates a large preview image.\n",
            "*   `<meta name=twitter:image content=\"https://numpy.org/images/numpy-image.jpg\">`\n",
            "*   `<meta name=twitter:title content=\"NumPy\">`\n",
            "*   `<meta name=twitter:description content=\"...\">`\n",
            "\n",
            "These tags provide information that social media platforms (like Twitter) use to generate previews when the page is shared.  They define the title, description, and image to display.\n",
            "\n",
            "**6. Navigation Bar:**\n",
            "\n",
            "*   `<nav id=nav class=navbar role=navigation aria-label=\"main navigation\">`: Defines the main navigation bar at the top of the page. It's likely styled with a CSS framework (like Bootstrap or Bulma) based on the `navbar` class.\n",
            "*   The `<a>` tags within the `<nav>` element create the navigation links: \"Install\", \"Documentation\", \"Learn\", \"Community\", \"About Us\", \"News\", \"Contribute\".\n",
            "*   There's also a language selector dropdown.\n",
            "\n",
            "**7. Hero Section:**\n",
            "\n",
            "*   `<section class=hero>`:  This is the main introductory section of the page, often featuring a large title, subtitle, and a call to action.\n",
            "*   It displays the NumPy logo, a subtitle (\"The fundamental package for scientific computing with Python\"), and a button linking to the latest release notes.\n",
            "\n",
            "**8. News Section:**\n",
            "\n",
            "*   `<div class=news-container>`:  Displays a brief announcement about a recent NumPy release (NumPy 2.3.0 released!).\n",
            "\n",
            "**9. Feature Cards:**\n",
            "\n",
            "*   `<section class=content-padding>`:  Contains a section with cards highlighting NumPy's key features (Powerful N-dimensional arrays, Numerical computing tools, Open source, etc.). These cards provide a concise overview of what NumPy offers.\n",
            "*   The code uses `sd-container-fluid`, `sd-row`, `sd-col`, `sd-card` classes, likely from the \"Sphinx Design\" extension, suggesting a grid-based layout.\n",
            "\n",
            "**10. Interactive Shell:**\n",
            "\n",
            "*   `<div class=hero-right>`: Contains a section to try NumPy in the browser.\n",
            "*   `<iframe class=numpy-shell src=\"https://jupyterlite.github.io/demo/repl/?toolbar=1&kernel=python&execute=0&code=...\"></iframe>`: Embeds an interactive Python shell using JupyterLite. This allows users to experiment with NumPy directly on the website without installing anything.  The `code=` parameter pre-populates the shell with example code.  This is a very effective way to engage visitors!\n",
            "\n",
            "**11. Ecosystem Tabs:**\n",
            "\n",
            "*   `<section class=tabs-section>`:  A section with tabbed content showcasing NumPy's ecosystem.\n",
            "*   The tabs are labeled \"Scientific Domains\", \"Array Libraries\", \"Data Science\", \"Machine Learning\", and \"Visualization\".  Each tab displays information about how NumPy is used in that area, along with links to relevant libraries and resources.\n",
            "*   The \"Scientific Domains\" tab has a list of subcategories and associated packages like \"Quantum Computing (QuTiP, PyQuil, Qiskit, PennyLane)\",  \"Statistical Computing (Pandas, statsmodels, Xarray, Seaborn)\" and more.  This showcases NumPy's widespread adoption.\n",
            "*   The \"Array Libraries\" tab lists other array computing libraries, highlighting NumPy's role as a foundation.\n",
            "*   The \"Data Science\", \"Machine Learning\", and \"Visualization\" tabs show how NumPy integrates with those areas.\n",
            "\n",
            "**12. Case Studies:**\n",
            "\n",
            "*   `<section class=casestudies>`:  Highlights real-world applications of NumPy through case studies (e.g., \"First Image of a Black Hole,\" \"Detection of Gravitational Waves\").  These are strong selling points.\n",
            "\n",
            "**13. Back to Top Button:**\n",
            "\n",
            "*   `<div id=backtotop>`:  Provides a \"back to top\" button.\n",
            "\n",
            "**14. Footer:**\n",
            "\n",
            "*   `<footer id=footer>`:  Contains the site's footer with links to various resources (Install, Documentation, About Us, etc.), a newsletter signup form, social media icons, and copyright information.\n",
            "*   The newsletter signup form uses Mailchimp.\n",
            "\n",
            "**Summary:**\n",
            "\n",
            "This HTML file is the code for the NumPy project's homepage.  It's generated using Hugo and relies heavily on the PyData Sphinx Theme and other CSS frameworks for styling. The page provides an overview of NumPy, its features, its ecosystem, and its applications. It also includes an interactive shell for users to try NumPy directly in their browsers.  The structure is well-organized, with clear sections for navigation, introduction, features, ecosystem, case studies, and contact information.  The site is also multilingual and has good social media integration.  The use of Sphinx suggests a strong focus on documentation and technical presentation. The numerous CSS files are all minified, meaning they were optimized for faster loading times.\n",
            "\n",
            "In short, it's a well-designed and informative homepage for a vital scientific computing library.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "92mWWiwbecGm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}